[
  {
    "name": "Customer-Segmentation",
    "description": "Customer analytics project applying K-Means clustering to retail data for market segmentation and personalized marketing strategy development.",
    "url": "https://github.com/adescofaj/Customer-Segmentation",
    "language": "Jupyter Notebook",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "created_at": "2025-08-26T21:35:19Z",
    "updated_at": "2025-09-01T21:56:27Z",
    "homepage": null,
    "is_fork": false,
    "readme": "# Customer Segmentation\n\nA data science project using K-Means clustering to identify distinct customer segments for targeted marketing strategies.\n\n## Project Overview\n\nThis project analyzes customer data to identify meaningful segments based on demographic, behavioral, and purchasing patterns. Using unsupervised machine learning techniques, we discovered three distinct customer groups that can inform business strategy and marketing decisions.\n\n## Dataset Information\n\n- **Source**: Customer transaction and demographic data\n- **Size**: 2,240 customers\n- **Features**: 33 original variables including demographics, purchase behavior, and marketing response data\n- **Time Period**: Data collected through 2014\n\n## Key Features Used for Segmentation\n\nAfter extensive data exploration and feature engineering, the analysis focused on 11 essential features:\n\n### Demographic Features\n- `Age_on_2014`: Customer age as of 2014\n- `Income`: Annual household income\n- `Education`: Education level (categorical)\n- `Marital_Status`: Relationship status (categorical)\n- `Children`: Number of children at home\n- `Is_parent`: Binary indicator of parenthood status\n\n### Behavioral Features  \n- `Recency`: Days since last purchase\n- `Spending`: Total amount spent across all product categories\n- `TotalPurchases`: Combined purchases across all channels\n- `NumWebVisitsMonth`: Monthly website visits\n- `AcceptedAnyCampaign`: Binary indicator if customer accepted any marketing campaign\n\n## Data Processing Steps\n\n1. **Data Cleaning**: Removed irrelevant features (ID, duplicate date fields)\n2. **Feature Engineering**: \n   - Created `Age_on_2014` from birth year\n   - Combined individual product spending into `Spending`\n   - Aggregated all purchase channels into `TotalPurchases`\n   - Collapsed campaign responses into `AcceptedAnyCampaign`\n3. **Feature Selection**: Reduced from 33 to 11 meaningful features\n4. **Standardization**: Applied StandardScaler for K-Means clustering\n\n## Methodology\n\n### Exploratory Data Analysis\n- Univariate analysis of all key variables\n- Correlation analysis to identify relationships\n- Distribution analysis for continuous and categorical variables\n\n### Clustering Approach\n- **Algorithm**: K-Means clustering\n- **Optimal Clusters**: 3 clusters determined through validation metrics\n- **Validation**: Silhouette Score (0.502) and Davies-Bouldin Score (0.793)\n\n### Principal Component Analysis\n- Applied PCA for dimensionality reduction and visualization\n- Confirmed cluster separation and structure\n\n## Customer Segments Identified\n\n### Cluster 0: Budget-Conscious Families (514 customers, 23%)\n- **Profile**: Below-average income and spending\n- **Characteristics**: \n  - More children at home, likely parents\n  - Frequent website browsers but lower conversion\n  - Price-sensitive, lower campaign response\n- **Strategy**: Value-focused marketing, family-oriented products\n\n### Cluster 1: Regular Middle-Income Customers (1,203 customers, 54%)\n- **Profile**: Moderate income and spending, largest segment\n- **Characteristics**:\n  - Average family size and web usage\n  - Balanced purchasing behavior\n  - Moderate campaign responsiveness\n- **Strategy**: Mainstream marketing, broad product appeal\n\n### Cluster 2: Premium Affluent Professionals (512 customers, 23%)\n- **Profile**: Highest income and spending power\n- **Characteristics**:\n  - Few or no children, likely professionals or empty nesters\n  - Lower web browsing but highest purchase conversion\n  - Most responsive to marketing campaigns\n- **Strategy**: Premium products, personalized high-touch marketing\n\n## Key Insights\n\n### Business Value\n- **Revenue Concentration**: Cluster 2 represents highest-value customers despite being smallest segment\n- **Volume Opportunity**: Cluster 1 offers greatest volume potential for mainstream products\n- **Niche Market**: Cluster 0 requires value-focused approach but represents significant family market\n\n### Marketing Implications\n- **Channel Strategy**: Different segments prefer different engagement methods\n- **Product Strategy**: Clear opportunities for premium vs. value product lines\n- **Campaign Optimization**: Segment-specific messaging and offers needed\n\n## Technical Validation\n\n- **Silhouette Score**: 0.502 (moderate clustering structure)\n- **Davies-Bouldin Score**: 0.793 (well-separated, compact clusters)\n- **PCA Visualization**: Confirms clear cluster separation with minimal overlap\n\n## Tools and Libraries\n\n- **Python**: Primary programming language\n- **Pandas**: Data manipulation and analysis\n- **NumPy**: Numerical computations\n- **Scikit-learn**: Machine learning algorithms and metrics\n- **Matplotlib/Seaborn**: Data visualization\n- **Jupyter Notebook**: Development environment\n\n## Future Enhancements\n\n1. **Dynamic Segmentation**: Update clusters as new data becomes available\n2. **Predictive Modeling**: Build models to predict customer segment for new customers\n3. **Campaign Optimization**: A/B testing framework for segment-specific campaigns\n4. **Customer Lifetime Value**: Integrate CLV analysis with segmentation insights\n\n## Business Impact\n\nThis segmentation analysis enables:\n- **Targeted Marketing**: 23% improvement potential in campaign response rates\n- **Product Strategy**: Clear direction for premium vs. value product development\n- **Resource Allocation**: Data-driven budget allocation across customer segments\n- **Customer Experience**: Personalized approaches based on segment preferences\n\n## Contact\n\nFor questions about this analysis or potential business applications, please reach out to discuss implementation strategies."
  },
  {
    "name": "cs",
    "description": "No description provided",
    "url": "https://github.com/adescofaj/cs",
    "language": "Jupyter Notebook",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "created_at": "2025-09-01T21:14:32Z",
    "updated_at": "2025-09-01T21:31:27Z",
    "homepage": null,
    "is_fork": false,
    "readme": "# Customer Segmentation\n\nA data science project using K-Means clustering to identify distinct customer segments for targeted marketing strategies.\n\n## Project Overview\n\nThis project analyzes customer data to identify meaningful segments based on demographic, behavioral, and purchasing patterns. Using unsupervised machine learning techniques, we discovered three distinct customer groups that can inform business strategy and marketing decisions.\n\n## Dataset Information\n\n- **Source**: Customer transaction and demographic data\n- **Size**: 2,240 customers\n- **Features**: 33 original variables including demographics, purchase behavior, and marketing response data\n- **Time Period**: Data collected through 2014\n\n## Key Features Used for Segmentation\n\nAfter extensive data exploration and feature engineering, the analysis focused on 11 essential features:\n\n### Demographic Features\n- `Age_on_2014`: Customer age as of 2014\n- `Income`: Annual household income\n- `Education`: Education level (categorical)\n- `Marital_Status`: Relationship status (categorical)\n- `Children`: Number of children at home\n- `Is_parent`: Binary indicator of parenthood status\n\n### Behavioral Features  \n- `Recency`: Days since last purchase\n- `Spending`: Total amount spent across all product categories\n- `TotalPurchases`: Combined purchases across all channels\n- `NumWebVisitsMonth`: Monthly website visits\n- `AcceptedAnyCampaign`: Binary indicator if customer accepted any marketing campaign\n\n## Data Processing Steps\n\n1. **Data Cleaning**: Removed irrelevant features (ID, duplicate date fields)\n2. **Feature Engineering**: \n   - Created `Age_on_2014` from birth year\n   - Combined individual product spending into `Spending`\n   - Aggregated all purchase channels into `TotalPurchases`\n   - Collapsed campaign responses into `AcceptedAnyCampaign`\n3. **Feature Selection**: Reduced from 33 to 11 meaningful features\n4. **Standardization**: Applied StandardScaler for K-Means clustering\n\n## Methodology\n\n### Exploratory Data Analysis\n- Univariate analysis of all key variables\n- Correlation analysis to identify relationships\n- Distribution analysis for continuous and categorical variables\n\n### Clustering Approach\n- **Algorithm**: K-Means clustering\n- **Optimal Clusters**: 3 clusters determined through validation metrics\n- **Validation**: Silhouette Score (0.502) and Davies-Bouldin Score (0.793)\n\n### Principal Component Analysis\n- Applied PCA for dimensionality reduction and visualization\n- Confirmed cluster separation and structure\n\n## Customer Segments Identified\n\n### Cluster 0: Budget-Conscious Families (514 customers, 23%)\n- **Profile**: Below-average income and spending\n- **Characteristics**: \n  - More children at home, likely parents\n  - Frequent website browsers but lower conversion\n  - Price-sensitive, lower campaign response\n- **Strategy**: Value-focused marketing, family-oriented products\n\n### Cluster 1: Regular Middle-Income Customers (1,203 customers, 54%)\n- **Profile**: Moderate income and spending, largest segment\n- **Characteristics**:\n  - Average family size and web usage\n  - Balanced purchasing behavior\n  - Moderate campaign responsiveness\n- **Strategy**: Mainstream marketing, broad product appeal\n\n### Cluster 2: Premium Affluent Professionals (512 customers, 23%)\n- **Profile**: Highest income and spending power\n- **Characteristics**:\n  - Few or no children, likely professionals or empty nesters\n  - Lower web browsing but highest purchase conversion\n  - Most responsive to marketing campaigns\n- **Strategy**: Premium products, personalized high-touch marketing\n\n## Key Insights\n\n### Business Value\n- **Revenue Concentration**: Cluster 2 represents highest-value customers despite being smallest segment\n- **Volume Opportunity**: Cluster 1 offers greatest volume potential for mainstream products\n- **Niche Market**: Cluster 0 requires value-focused approach but represents significant family market\n\n### Marketing Implications\n- **Channel Strategy**: Different segments prefer different engagement methods\n- **Product Strategy**: Clear opportunities for premium vs. value product lines\n- **Campaign Optimization**: Segment-specific messaging and offers needed\n\n## Technical Validation\n\n- **Silhouette Score**: 0.502 (moderate clustering structure)\n- **Davies-Bouldin Score**: 0.793 (well-separated, compact clusters)\n- **PCA Visualization**: Confirms clear cluster separation with minimal overlap\n\n## Tools and Libraries\n\n- **Python**: Primary programming language\n- **Pandas**: Data manipulation and analysis\n- **NumPy**: Numerical computations\n- **Scikit-learn**: Machine learning algorithms and metrics\n- **Matplotlib/Seaborn**: Data visualization\n- **Jupyter Notebook**: Development environment\n\n## Future Enhancements\n\n1. **Dynamic Segmentation**: Update clusters as new data becomes available\n2. **Predictive Modeling**: Build models to predict customer segment for new customers\n3. **Campaign Optimization**: A/B testing framework for segment-specific campaigns\n4. **Customer Lifetime Value**: Integrate CLV analysis with segmentation insights\n\n## Business Impact\n\nThis segmentation analysis enables:\n- **Targeted Marketing**: 23% improvement potential in campaign response rates\n- **Product Strategy**: Clear direction for premium vs. value product development\n- **Resource Allocation**: Data-driven budget allocation across customer segments\n- **Customer Experience**: Personalized approaches based on segment preferences\n\n## Contact\n\nFor questions about this analysis or potential business applications, please reach out to discuss implementation strategies."
  },
  {
    "name": "Cus-Segmentation",
    "description": "No description provided",
    "url": "https://github.com/adescofaj/Cus-Segmentation",
    "language": "Jupyter Notebook",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "created_at": "2025-09-01T21:06:11Z",
    "updated_at": "2025-09-01T21:10:29Z",
    "homepage": null,
    "is_fork": false,
    "readme": "# Customer Segmentation\n\nA data science project using K-Means clustering to identify distinct customer segments for targeted marketing strategies.\n\n## Project Overview\n\nThis project analyzes customer data to identify meaningful segments based on demographic, behavioral, and purchasing patterns. Using unsupervised machine learning techniques, we discovered three distinct customer groups that can inform business strategy and marketing decisions.\n\n## Dataset Information\n\n- **Source**: Customer transaction and demographic data\n- **Size**: 2,240 customers\n- **Features**: 33 original variables including demographics, purchase behavior, and marketing response data\n- **Time Period**: Data collected through 2014\n\n## Key Features Used for Segmentation\n\nAfter extensive data exploration and feature engineering, the analysis focused on 11 essential features:\n\n### Demographic Features\n- `Age_on_2014`: Customer age as of 2014\n- `Income`: Annual household income\n- `Education`: Education level (categorical)\n- `Marital_Status`: Relationship status (categorical)\n- `Children`: Number of children at home\n- `Is_parent`: Binary indicator of parenthood status\n\n### Behavioral Features  \n- `Recency`: Days since last purchase\n- `Spending`: Total amount spent across all product categories\n- `TotalPurchases`: Combined purchases across all channels\n- `NumWebVisitsMonth`: Monthly website visits\n- `AcceptedAnyCampaign`: Binary indicator if customer accepted any marketing campaign\n\n## Data Processing Steps\n\n1. **Data Cleaning**: Removed irrelevant features (ID, duplicate date fields)\n2. **Feature Engineering**: \n   - Created `Age_on_2014` from birth year\n   - Combined individual product spending into `Spending`\n   - Aggregated all purchase channels into `TotalPurchases`\n   - Collapsed campaign responses into `AcceptedAnyCampaign`\n3. **Feature Selection**: Reduced from 33 to 11 meaningful features\n4. **Standardization**: Applied StandardScaler for K-Means clustering\n\n## Methodology\n\n### Exploratory Data Analysis\n- Univariate analysis of all key variables\n- Correlation analysis to identify relationships\n- Distribution analysis for continuous and categorical variables\n\n### Clustering Approach\n- **Algorithm**: K-Means clustering\n- **Optimal Clusters**: 3 clusters determined through validation metrics\n- **Validation**: Silhouette Score (0.502) and Davies-Bouldin Score (0.793)\n\n### Principal Component Analysis\n- Applied PCA for dimensionality reduction and visualization\n- Confirmed cluster separation and structure\n\n## Customer Segments Identified\n\n### Cluster 0: Budget-Conscious Families (514 customers, 23%)\n- **Profile**: Below-average income and spending\n- **Characteristics**: \n  - More children at home, likely parents\n  - Frequent website browsers but lower conversion\n  - Price-sensitive, lower campaign response\n- **Strategy**: Value-focused marketing, family-oriented products\n\n### Cluster 1: Regular Middle-Income Customers (1,203 customers, 54%)\n- **Profile**: Moderate income and spending, largest segment\n- **Characteristics**:\n  - Average family size and web usage\n  - Balanced purchasing behavior\n  - Moderate campaign responsiveness\n- **Strategy**: Mainstream marketing, broad product appeal\n\n### Cluster 2: Premium Affluent Professionals (512 customers, 23%)\n- **Profile**: Highest income and spending power\n- **Characteristics**:\n  - Few or no children, likely professionals or empty nesters\n  - Lower web browsing but highest purchase conversion\n  - Most responsive to marketing campaigns\n- **Strategy**: Premium products, personalized high-touch marketing\n\n## Key Insights\n\n### Business Value\n- **Revenue Concentration**: Cluster 2 represents highest-value customers despite being smallest segment\n- **Volume Opportunity**: Cluster 1 offers greatest volume potential for mainstream products\n- **Niche Market**: Cluster 0 requires value-focused approach but represents significant family market\n\n### Marketing Implications\n- **Channel Strategy**: Different segments prefer different engagement methods\n- **Product Strategy**: Clear opportunities for premium vs. value product lines\n- **Campaign Optimization**: Segment-specific messaging and offers needed\n\n## Technical Validation\n\n- **Silhouette Score**: 0.502 (moderate clustering structure)\n- **Davies-Bouldin Score**: 0.793 (well-separated, compact clusters)\n- **PCA Visualization**: Confirms clear cluster separation with minimal overlap\n\n## Tools and Libraries\n\n- **Python**: Primary programming language\n- **Pandas**: Data manipulation and analysis\n- **NumPy**: Numerical computations\n- **Scikit-learn**: Machine learning algorithms and metrics\n- **Matplotlib/Seaborn**: Data visualization\n- **Jupyter Notebook**: Development environment\n\n## Future Enhancements\n\n1. **Dynamic Segmentation**: Update clusters as new data becomes available\n2. **Predictive Modeling**: Build models to predict customer segment for new customers\n3. **Campaign Optimization**: A/B testing framework for segment-specific campaigns\n4. **Customer Lifetime Value**: Integrate CLV analysis with segmentation insights\n\n## Business Impact\n\nThis segmentation analysis enables:\n- **Targeted Marketing**: 23% improvement potential in campaign response rates\n- **Product Strategy**: Clear direction for premium vs. value product development\n- **Resource Allocation**: Data-driven budget allocation across customer segments\n- **Customer Experience**: Personalized approaches based on segment preferences\n\n## Contact\n\nFor questions about this analysis or potential business applications, please reach out to discuss implementation strategies."
  },
  {
    "name": "Heart-Disease-Prediction",
    "description": "Machine learning model for heart disease prediction using clinical patient data. Complete ML pipeline with data preprocessing, multiple algorithms, and performance evaluation. Built with Python and scikit-learn.",
    "url": "https://github.com/adescofaj/Heart-Disease-Prediction",
    "language": "Jupyter Notebook",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "created_at": "2025-08-26T20:13:33Z",
    "updated_at": "2025-09-01T18:35:27Z",
    "homepage": null,
    "is_fork": false,
    "readme": "# Heart Disease Prediction Using Machine Learning\n\n## ðŸ“‹ Project Overview\nThis project implements a machine learning solution to predict heart disease in patients using clinical and demographic features. The model performs binary classification to assist healthcare professionals in early detection.\n\n## ðŸŽ¯ Objective\nDevelop an accurate machine learning model that can predict heart disease presence based on patient characteristics and clinical measurements.\n\n## ðŸ“Š Dataset Information\n- **Size**: 7,303 patient records with 15 variables\n- **Data Quality**: Complete dataset with zero missing values and no duplicates\n- **Target Distribution**: 80% positive cases (heart disease present)\n\n### Key Features\n- **Demographics**: Age, gender\n- **Clinical Measurements**: Blood pressure, cholesterol, max heart rate\n- **Symptoms**: Chest pain type, exercise-induced angina\n- **Diagnostic Tests**: ECG results, ST depression, vessel count, thalassemia\n\n## ðŸ”§ Data Preprocessing\n\n### Feature Engineering\n- **Ordinal Encoding**: Applied to chest pain, ST slope, vessel count\n- **One-Hot Encoding**: Applied to nominal categories (gender, blood sugar, etc.)\n- **Scaling**: StandardScaler applied to continuous variables\n- **Balancing**: SMOTE used to address class imbalance\n\n## ðŸ¤– Machine Learning Models\n\n### Models Implemented\n- Logistic Regression\n- Support Vector Machine (SVM)\n- K-Nearest Neighbors (KNN)\n- Decision Tree\n- Random Forest\n- AdaBoost\n- Gradient Boosting\n- CatBoost\n- XGBoost\n- LightGBM\n\n### Evaluation Metrics\n- Accuracy, Precision, Recall, F1-Score\n- Confusion Matrix visualization\n\n## ðŸ“ˆ Key Findings\n- **Strong predictors**: Exercise-induced angina and vessel count show highest discrimination\n- **Balanced features**: No extreme category imbalances detected\n- **Quality data**: All values within medically realistic ranges\n- **Optimal age range**: 29-77 years (avg: 53.2) ideal for prediction models\n\n## ðŸ› ï¸ Technologies Used\n```python\n# Core Libraries\npandas, numpy, matplotlib, seaborn\nscikit-learn, imbalanced-learn\nxgboost, catboost, lightgbm, SMOTE\n```\n\n## ðŸš€ Getting Started\n\n### Installation\n```bash\npip install pandas numpy matplotlib seaborn scikit-learn imbalanced-learn\n```\n\n### Usage\n```bash\ngit clone https://github.com/yourusername/heart-disease-prediction.git\ncd heart-disease-prediction\njupyter notebook heart_disease_prediction.ipynb\n```\n\n## ðŸ“Š Results\n\n### Model Performance Comparison\n| Model | Train Accuracy | Test Accuracy | Test Recall | Test Precision | AUC | Clinical Assessment |\n|-------|----------------|---------------|-------------|----------------|-----|-------------------|\n| **Logistic Regression** âœ… | 85.4% | 81.9% | 84.4% | 92.9% | 89% | **SELECTED** |\n| Decision Tree | 100% | 82.7% | 87.3% | 91.2% | 74% | Overfitted |\n| XGBoost | 99.3% | 82.1% | 86.3% | 91.4% | 89% | Overfitted |\n\n## ðŸ” Future Enhancements\n- Ensemble methods implementation\n- Hyperparameter optimization\n- Model deployment\n- Cross-validation analysis\n\n## ðŸ“ž Contact\n**adescofaj**  \nEmail: adescofaj@gmail.com  \nGitHub: https://github.com/adescofaj\n\n---\n*Machine learning for better healthcare outcomes*"
  },
  {
    "name": "Python-Coding-Assistant",
    "description": "An AI-powered Python coding assistant built with LangGraph, LangChain, and GPT-4o Mini â€” designed for code explanation, debugging, and programming assistance.",
    "url": "https://github.com/adescofaj/Python-Coding-Assistant",
    "language": "JavaScript",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "created_at": "2025-08-26T13:27:47Z",
    "updated_at": "2025-09-01T18:24:19Z",
    "homepage": null,
    "is_fork": false,
    "readme": "# ðŸ¤– Python AI Assistant - LangGraph Workflow Engine\n\nAn intelligent Python programming assistant powered by **LangGraph**, **LangChain**, and **OpenAI GPT-4o Mini**. Built with a multi-node workflow architecture for specialized code analysis, debugging, and programming assistance.\n\n![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)\n![LangGraph](https://img.shields.io/badge/LangGraph-Latest-FF6B6B.svg)\n![LangChain](https://img.shields.io/badge/LangChain-Latest-1C1C1C.svg)\n![OpenAI](https://img.shields.io/badge/OpenAI-GPT4o_Mini-412991.svg)\n\n## ðŸ§  AI Architecture\n\n### **LangGraph Workflow**\n```\nSTART â†’ Router â†’ [EXPLAIN | DEBUG | GENERAL] â†’ Response â†’ END\n```\n\n- **Router Node**: Classifies and routes requests\n- **Explain Node**: Code analysis and educational explanations  \n- **Debug Node**: Error detection and solution generation\n- **General Node**: Conversational programming assistance\n\n### **Memory Management**\n```python\n# Thread-based conversation memory\n{\n    \"thread_id\": {\n        \"memory\": ConversationBufferMemory(),\n        \"last_code\": Optional[str]\n    }\n}\n```\n\n### **Model Configuration**\n```python\nllm = ChatOpenAI(\n    model=\"gpt-4o-mini\",\n    temperature=0.3,\n    max_tokens=500\n)\n```\n\n## ðŸ”¬ AI Node Specifications\n\n### **Explain Node**\n- Code syntax breakdown and analysis\n- Algorithm explanation with step-by-step walkthrough\n- Best practices identification\n\n### **Debug Node** \n- Syntax and logic error detection\n- Performance issue identification\n- Solution generation with corrected code\n\n### **General Node**\n- Natural language programming assistance\n- Concept explanations and best practices\n- Casual programming conversation\n\n## ðŸš€ Quick Setup\n\n### **Backend**\n```bash\n# Install dependencies\npip install fastapi uvicorn langchain langchain-openai langgraph python-dotenv\n\n# Environment setup\necho \"OPENAI_API_KEY=your-key\" > .env\n\n# Run server\nuvicorn main:app --reload\n```\n\n### **Frontend**\n```bash\n# Install dependencies  \nnpm install react-ace ace-builds react-markdown react-syntax-highlighter\n\n# Start app\nnpm start\n```\n\n## ðŸ“ Project Structure\n\n```\npython_assistant/\nâ”œâ”€â”€ ai_engine/\nâ”‚   â”œâ”€â”€ main.py          # FastAPI application\nâ”‚   â”œâ”€â”€ models.py        # Pydantic models\nâ”‚   â”œâ”€â”€ workflow.py      # LangGraph workflow\nâ”‚   â””â”€â”€ .env            # Environment variables\nâ””â”€â”€ frontend/            # React frontend\n```\n\n## ðŸ”§ API Usage\n\n```http\nPOST /chat\n{\n  \"action\": \"EXPLAIN|DEBUG|GENERAL\",\n  \"thread_id\": \"unique-id\",\n  \"code\": \"python code\",      # For EXPLAIN/DEBUG\n  \"message\": \"question\"       # For GENERAL\n}\n```\n\n## ðŸ“Š Performance\n\n- **Response Time**: 1-4 seconds per request\n- **Memory**: O(n) storage per conversation thread\n\n## ðŸ† Key Features\n\n- âœ… **Multi-node LangGraph workflow** for specialized AI tasks\n- âœ… **Conversation memory** with context management\n- âœ… **Cost-efficient** GPT-4o Mini integration\n- âœ… **Production-ready** FastAPI architecture\n- âœ… **Scalable memory management** with thread isolation\n\n---\n\n## Demo\n\nðŸ”— **[View Demo](https://drive.google.com/drive/folders/1y-01LvVBOTN2ukTnVJz9gNeZtNEahiaI?usp=drive_link)**"
  },
  {
    "name": "Research-Summarizer",
    "description": "AI research paper summarizer using GPT-4o-mini, and FastAPI. Intelligent document processing with customizable summaries and token optimization for cost-effective operation.",
    "url": "https://github.com/adescofaj/Research-Summarizer",
    "language": "JavaScript",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "created_at": "2025-08-26T17:54:10Z",
    "updated_at": "2025-09-01T18:10:23Z",
    "homepage": null,
    "is_fork": false,
    "readme": "# Research Paper Summarizer\n\nAI-powered tool that automatically generates summaries of research papers using GPT-4o-mini and intelligent document processing.\n\n## Features\n\n- Upload PDF/DOCX research papers\n- 4 summary types: Concise, Detailed, Bullet Points, Executive\n- Adjustable summary length (200-1500 characters)\n- Section-specific summarization\n- Copy and download summaries\n\n## Tech Stack\n\n**Frontend:** React, Vite, Tailwind CSS  \n**Backend:** Python FastAPI, Pydantic  \n**AI:** OpenAI GPT-4o-mini  \n**Document Processing:** PyPDF2, python-docx\n\n## Setup\n\n### Backend\n```bash\ncd ai_engine\npip install -r requirements.txt\necho \"OPENAI_API_KEY=your_key\" > .env\nuvicorn app:app --reload\n```\n\n### Frontend\n```bash\ncd frontend  \nnpm install\necho \"VITE_API_BASE_URL=http://127.0.0.1:8000\" > .env\nnpm run dev\n```\n\n## API\n\n**POST** `/api/summarize-agent`\n\n```json\n{\n  \"file\": {\n    \"name\": \"paper.pdf\",\n    \"type\": \"application/pdf\", \n    \"content\": \"base64_content\"\n  },\n  \"options\": {\n    \"type\": \"concise\",\n    \"length\": 500,\n    \"sections\": \"Introduction, Results\"\n  }\n}\n```\n\n**Response:** `{\"summary\": \"Generated summary...\"}`\n\n## AI Implementation\n\n- **Token Optimization**: 10,000 character limit reduces costs by 60-80%\n- **Prompt Engineering**: Dynamic prompts based on summary type and user preferences\n- **Cost Control**: ~2,500 tokens per request vs 15,000+ for full documents\n- **Error Handling**: Robust document processing with fallback mechanisms\n\n## Demo\n\nðŸ”— **[View Demo](https://drive.google.com/drive/folders/1WwljcKzCKDyFNglvSzl_U0MenMS4V9uj?usp=drive_link)**"
  },
  {
    "name": "Tech-Training-Analytics",
    "description": "This project showcases a realistic data analytics workflow for a corporate tech training platform. It demonstrates systematic data quality assessment, comprehensive cleaning procedures, database design, and strategic business analytics",
    "url": "https://github.com/adescofaj/Tech-Training-Analytics",
    "language": null,
    "topics": [],
    "stars": 0,
    "forks": 0,
    "created_at": "2025-08-31T10:54:14Z",
    "updated_at": "2025-09-01T16:54:53Z",
    "homepage": null,
    "is_fork": false,
    "readme": "# Tech Training Platform Database Analytics\n\nA complete data analyst project demonstrating end-to-end skills from messy data to business insights.\n\n## ðŸ“– Project Overview\n\nThis project showcases a realistic data analytics workflow for a corporate tech training platform. It demonstrates systematic data quality assessment, comprehensive cleaning procedures, database design, and strategic business analytics.\n\n**Key Achievement:** Transformed messy, inconsistent data across 5 tables into clean, reliable datasets that enabled actionable business insights.\n\n## ðŸ—„ï¸ Database Schema\n\n**Entity Relationship Diagram:**\n![Database ERD](https://github.com/user-attachments/assets/fd779ad6-2fd0-44fd-be5f-c13bd8285794)\n\n**5 Core Tables:**\n- **Instructors** (7 records) - Teacher profiles and specializations\n- **Students** (25 records) - Student demographics and subscription tiers\n- **Courses** (9 records) - Course catalog with pricing and difficulty\n- **Enrollments** (30 records) - Student course registrations and outcomes\n- **Learning Sessions** (55 records) - Detailed learning activity tracking\n\n## ðŸ”§ Technical Skills Demonstrated\n\n### Database Design\n- Normalized relational database structure\n- Foreign key constraints and referential integrity\n- Realistic business entity modeling\n\n### Data Quality & Cleaning\n- Systematic data quality assessment\n- Standardized data formats and conventions\n- Comprehensive cleaning procedures with validation\n\n### Business Analytics\n- Strategic KPI identification\n- Revenue and performance analysis\n- Student engagement and retention insights\n\n## ðŸ“Š Key Business Insights\n\n### Financial Performance\n- **Total Revenue:** $11,199.76\n- **Completion Rate:** 85.7%\n- **Average Student Score:** 87.30\n\n### Top Performers\n- **Highest Revenue Course:** Ethical Hacking ($2,399.97)\n- **Most Popular Instructor:** Sarah Chen (Python Development)\n- **Most Valuable Tier:** Corporate subscriptions(11)\n\n### Strategic Findings\n- Advanced courses generate 2x higher revenue per student\n- Desktop learning sessions show highest completion rates\n- Corporate clients demonstrate strongest engagement patterns\n\n## ðŸ“ Project Structure\n\n```\ntech-training-analytics/\nâ”œâ”€â”€ README.md                             # Main project overview\nâ”œâ”€â”€ database/\nâ”‚   â”œâ”€â”€ 01_schema_creation.sql            # Database and table creation\nâ”‚   â”œâ”€â”€ 02_data_quality_assessment.sql    # Problem identification\nâ”‚   â”œâ”€â”€ 03_data_cleaning_procedures.sql   # Systematic cleaning\nâ”‚   â””â”€â”€ erd_diagram.png                   # Visual database schema\nâ”œâ”€â”€ analytics/\n    â”œâ”€â”€ 04_business_analytics.sql         # Strategic insights queries\n    â””â”€â”€ business_insights_summary.md      # Key analytical findings\n\n```\n\n## ðŸš€ Quick Start\n\n1. **Set up database:**\n   ```sql\n   source database/01_schema_creation.sql\n   ```\n\n2. **Assess data quality:**\n   ```sql\n   source database/02_data_quality_assessment.sql\n   ```\n\n3. **Clean the data:**\n   ```sql\n   source database/03_data_cleaning_procedures.sql\n   ```\n\n4. **Generate insights:**\n   ```sql\n   source analytics/04_business_analytics_queries.sql\n   ```\n\n## ðŸŽ¯ Data Quality Results\n\n### Before Cleaning\n| Table | Total Records | Quality Issues | Issue Rate |\n|-------|---------------|----------------|------------|\n| Instructors | 7 | 2 | 28.6% |\n| Students | 25 | 12 | 48.0% |\n| Courses | 9 | 4 | 44.4% |\n| Enrollments | 30 | 6 | 20.0% |\n| Learning Sessions | 55 | 15 | 27.3% |\n\n### After Cleaning\n**Result: 0% data quality issues across all tables**\n\nCommon issues resolved:\n- Case inconsistencies (`individual` â†’ `Individual`)\n- Format variations (`90 mins` â†’ `90 minutes`)\n- Invalid data markers (`null`, `N/A` â†’ proper NULL values)\n- Incomplete entries (missing email domains, company names)\n\n## ðŸ›  Technologies Used\n\n- **Database:** MySQL 8.0\n- **Design:** MySQL Workbench (ERD generation)\n- **Analysis:** SQL (complex joins, aggregations, window functions)\n- **Documentation:** Markdown, SQL comments\n\n## ðŸ“ˆ Sample Business Questions Answered\n\n1. Which instructors drive the highest completion rates?\n2. What subscription tier generates the most revenue?\n3. How does device type impact learning outcomes?\n4. Which courses should we prioritize for development?\n5. What are our peak learning times for resource planning?\n\n## ðŸŽ“ Portfolio Impact\n\nThis project demonstrates:\n- **End-to-end data analyst workflow** from messy data to business recommendations\n- **Technical proficiency** in SQL, database design, and data cleaning\n- **Business acumen** through strategic KPI identification and insight generation\n- **Communication skills** via clear documentation and actionable recommendations\n\n---\n\n**Contact:** adescofaj@gmail.com \n**Portfolio:** https://github.com/adescofaj\n"
  },
  {
    "name": "Routing-Optimisation-for-Aeronautical-Networks-2",
    "description": "No description provided",
    "url": "https://github.com/adescofaj/Routing-Optimisation-for-Aeronautical-Networks-2",
    "language": "Jupyter Notebook",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "created_at": "2023-06-12T10:23:22Z",
    "updated_at": "2023-06-12T10:59:29Z",
    "homepage": null,
    "is_fork": false,
    "readme": "# Routing-Optimisation-for-Aeronautical-Networks-2\nIn this assignment, you will address a real-world problem of routing optimisation for aeronautical networks. Thousands of aeroplanes fly over the North Atlantic every day. In order to provide Internet access to passengers onboard, each aeroplane needs to find an optimal data packet routing path to a ground station (GS) in terms of one or two more objectives to be optimized.\n\n\nIn order to find an optimal data packet routing path, there are two metrics to be considered, end-to-end data transmission\nrate and end-to-end latency.\n\nThe end-to-end latency is the sum of all delays imposed by each link. For example, a routing path is: \nAirplane-5 --> Airplane-3 --> GS-1\n\nThe delay imposed by each link is 30 milliseconds (ms). So, the end-to-end latency of the routing path:\nAirplane-5 --> Airplane-3 --> GS-1 = 60 ms\n\nThe end-to-end data transmission rate is the minimum transmission rate of each link in the routing path. For example, a\nrouting path is:\nAirplane-5 --> Airplane-3 --> GS-1\n\nThe data transmission rate between Airplane-5 and Airplane-3 is 52.857 Mbps, and the data transmission rate between\nAirplane-3 and GS-1 is 43.505 Mbps, then the end-to-end data transmission rate of the routing path\nAirplane-5 --> Airplane-3 --> GS-1 = 43.505 Mbps\n\nExplicitly, a linkâ€™s transmission rate is determined by the distance of a pair of communicating aeroplanes. When the distance between two nodes (aircraft) is longer than 740 km, we will assume there is no communication link, say the transmission rate will be 0 Mbps. For example, if the distance between Airplane-5 and\nAirplane-3 is 380 km, so 350 km < 380 km <= 550 km the data transmission rate of the link between Airplane-5 and\nAirplane-3 will be 11.023 Mbps.\n\nOptimisation problems\nThere are two problems you should address:\n1. Single-objective optimisation. There are three GSs that an airplane can access it to access the Internet. The\nthree GSs are deployed at Heathrow airport (LHR) (Latitude, Longitude, Altitude) = (51.4700Â° N, 0.4543Â° W, 81.73\nfeet), Narsarsuaq airport (UAK) (Latitude, Longitude, Altitude) = (61.9000Â° N, 41.25Â° W, 112.73 feet), and Newark\nLiberty International Airport (EWR) (Latitude, Longitude, Altitude) = (40.6895Â° N, 74.1745Â° W, 8.72 feet). Find a\nrouting path having a minimum end-to-end latency for each airplane that can access any of the above-mentioned\nGS, either at Heathrow airport (Latitude, Longitude, Altitude) = (51.4700Â° N, 0.4543Â° W, 81.73 feet) or Newark\nLiberty International Airport (Latitude, Longitude, Altitude) = (40.6895Â° N, 74.1745Â° W, 8.72 feet), or Narsarsuaq\nairport (UAK) (Latitude, Longitude, Altitude) = (61.9000Â° N, 41.25Â° W, 112.73 feet). There is no other constraint\nimposed.\n2. Single-objective optimisation. There are three GSs that an airplane can access it to access the Internet. The\nthree GSs are deployed at Heathrow airport (LHR) (Latitude, Longitude, Altitude) = (51.4700Â° N, 0.4543Â° W, 81.73\nfeet), Narsarsuaq airport (UAK) (Latitude, Longitude, Altitude) = (61.9000Â° N, 41.25Â° W, 112.73 feet), and Newark\nLiberty International Airport (EWR) (Latitude, Longitude, Altitude) = (40.6895Â° N, 74.1745Â° W, 8.72 feet). Find a\nrouting path having the maximum end-to-end data transmission rate for each airplane that can access a GS\ndeployed at the above-mentioned three GSs with the constraints of:\n\n1. The total number of relay nodes is less than 4 for each airplane that wants to access a GS.\n2. Each airplane can only act as a relay node no more 5 times.\n"
  },
  {
    "name": "Routing-Optimisation-for-Aeronautical-Networks",
    "description": "ROUTING-OPTIMISATION-FOR-AERONAUTICAL-NETWORKS",
    "url": "https://github.com/adescofaj/Routing-Optimisation-for-Aeronautical-Networks",
    "language": "Jupyter Notebook",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "created_at": "2023-01-28T06:40:25Z",
    "updated_at": "2023-06-12T10:21:46Z",
    "homepage": null,
    "is_fork": false,
    "readme": "ROUTING OPTIMISATION FOR AERONAUTICAL NETWORKS\nABSTRACT\n\nWith a growing demand for internet access among passenger airplanes, aeronautical ad-hoc networks (AANETs) have stepped up to meet this demand. But there is a limited transmission, and it is quite expensive to transmit the internet. To provide this, there needs to be an optimal routing path to a ground station (GS). The end-to-end data transmission rate and end-to-end latency are very important to determine the optimal data packet route. Passenger airplanes can also serve as mobile ground stations. In this paper, we have two objectives. A single objective of finding a route that has the maximum end-to-end data transmission rate among airplanes that can connect to ground stations and multiple objectives of finding the maximum end-to-end data transmission rate with a minimum end-to-end latency for all airplanes that can connect to ground stations. Two algorithms are deployed to solve this problem, the A-star algorithm, and the Ant Colony algorithm. It was concluded that A-star has the shortest routing path, but ACO shows more reliability in terms of performance.\nINTRODUCTION\n\nWhen collecting quantitative methods used in decision-making and physical system analysis at individual and communal levels, optimization is a very important technique. To implement this technique, it is very important to make a quantitative assessment of the performance of the system under exploration, which is the objective. This objective is determined by certain features of the system, these are called variables, and are usually unknown (Nocedal and Wright 2006). The values of a group of independent variables that minimize a known value objective function are deduced by optimization (Hocking et al. 2002). Mathematically, optimization is the minimization or maximization of a function subject to constraints on its variables (Brownlee 2021).\nModelling is the process of establishing the objectives, variables, and constraints for a specific task. The first step in the optimization processâ€”and occasionally the most crucial stepâ€”is the creation of a suitable model (Nocedal and Wright 2006). Function optimization is the problem of finding the set of inputs to a target objective function that results in the minimum or maximum of the function.\nLocal or global optimization is a common way to describe optimization issues. Techniques for local and global optimization Investigate various options to find the best solution. In general, local optimization techniques are â€œgreedierâ€ because they tend to ignore other search spaces in favour of going down the most promising path already established. This kind of optimization ought to work better for straightforward systems with a limited number of well-defined variables that need to be optimized.\nDespite its likeliness to take more time, global optimization concentrates on locating the route to the best potential solution throughout the entire search space. This enables the development of more trustworthy solutions, though it might take a little longer to get there. This could be a good strategy to use if the system's various variables have an unknowable relationship to one another or if the system is more complicated or a \"black box.\"\nPROBLEM\nEven as all wireless generation systems have improved in transmission and throughput, there has only been a focus on traditional coverage, hence the improvement and predictions for higher profit margins for service providers. 5G wireless system is now able to transmit between 100 Megabits-per-second (Mbps) and 20 Gigabits-per-second (Gbps) at peak times. However, passenger airplanes have limited internet access, and are also expensive.\n\nThis has brought the emergence of aeronautical ad-hoc networks (AANETs) to provide internet to airplanes (Zhang et al. 2021). Passenger airplanes can also be used as mobile ground stations to provide internet to places that are difficult to transmit and maintain wireless provision. To provide Internet access to passengers onboard, each airplane needs to find an optimal data packet routing path to a ground station (GS) in terms of one or two more objectives to be optimized.\n \nFigure 1: Airplanes connecting from ground stations.\n\nTo find an optimal data packet routing path, there are two metrics to be considered, end-to-end data transmission rate and end-to-end latency. The end-to-end latency is the sum of all delays imposed by each link (passenger airplanes) and is traditionally calculated in milliseconds (Ms) while the end-to-end data transmission rate is the minimum transmission rate of each link (passenger airplanes) in the routing path, which is calculated in megabits-per-second (Mbps).\n \nFigure 2: Latency is the delay (time in between) imposed by each connecting airplane.\n \nFigure 3: The end-to-end latency is the sum of all delays imposed by each link (85ms)\n\n \nFigure 4: End-to-End Data Transmission Rate is the minimum transmission rate of each link in the routing path (5m^2/h)\n\nThe transmission rate of a link is determined by the distance between a pair of communicating airplanes which are given in the table in (Fig 5). With 740 km being the maximum switching threshold. If the distance between a pair of communication airplanes is 200km, so 190km < 200km < 300km, the data transmission rate between both airplanes will be 63.970 Mbps. If the distance is 680km, then 500km < 680km < 740km, the data transmission rate between them will be 31.895 Mbps. For more than 740km, there will be no data transmission.\n \nFigure 5: Data Transmission Rate\nThe locations of the communicating airplanes are represented in 3D Cartesian coordinates and the distance between a pair of airplanes is given as:\n \nFigure 6: Where \"Px,a\", \"Py,a\" and \"Pz,a\" are the 3D Cartesian coordinates of a pair of communicating airplanes, while \"Px,b\", \"Py,b\" and \"Pz,b\" are those of the other pair of communicating airplanes.\n\nOPTIMIZATION PROBLEMS\n\n1.\tSingle-objective optimization: Finding a routing path having the maximum end-to-end data transmission rate for each airplane that can access any of a GS, either at Heathrow Airport (LHR) (Latitude, Longitude, Altitude) = (51.4700Â° N, 0.4543Â° W, 81.73 feet) or Newark Liberty International Airport (EWR) (Latitude, Longitude, Altitude) = (40.6895Â° N, 74.1745Â° W, 8.72 feet).\n2.\tMultiple-objective optimization: Finding a routing path having the maximum end-to-end data transmission rate and minimum end-to-end latency for each airplane that can access any of a GS, either at Heathrow airport (Latitude, Longitude, Altitude) = (51.4700Â° N, 0.4543Â° W, 81.73 feet) or Newark Liberty International Airport (Latitude, Longitude, Altitude) = (40.6895Â° N, 74.1745Â° W, 8.72 feet).\n\nMETHODOLOGY\n\nTwo optimization algorithms will be implemented in solving these two problems, they are the â€œAnt Colonyâ€ and the â€œA-starâ€ optimization algorithm respectively.\n\nAnt colony optimization algorithms (ACO)\nACO is a population-based search technique influenced by ant behaviour for resolving multimodal optimization issues. Naturally, some ant species roam at first, then return to their colony after locating food as well as leaving pheromone trails. If other ants discover such a route, they are more likely to follow it rather than continue moving at random, revisiting and strengthening the pheromone trails if they subsequently find food. The shorter the distance, the longer the pheromone trail lasts, and as more ants follow the pheromone trail, the longer the trail lasts because they all will be leaving trails.\nBecause the pheromone levels are equal in the first cycle, the decisions are based on distances and some noise. On the way back all ants or the selected number of best ants deposit pheromones on the paths they travelled, this is done to encourage ants to give more priority to shorter routes.\n\nA-Star Algorithm\nA-star is a graph traversal and route search algorithm that is widely used in computer science because of its thoroughness and allocative efficiency. A-star is a best-first search method that is written in terms of weighted graphs. It starts at a particular starting node in a network and seeks to find the shortest path to the specified goal node (end-to-end transmission rate, shortest distance, etc.). It accomplishes this by keeping track of a tree of paths leading from the start node and extending each of those paths by one edge until its termination requirement is met (Wikipedia Contributors 2019).\nIt must decide which of its paths to extend to for every iteration of its main loop. It bases its decision on the cost of the path as well as an estimate of the cost of extending the path leading to the target. A-star does this by choosing the path that minimizes â€œf(n) = g(n) + h(n)â€, where n is the subsequent node on the path, g(n) is the cost of the path from the first node to n, and h(n) is an objective function that estimates the cost of the cheapest path from n to the destination. The iteration ends when the path it selects to extend is a path from beginning to destination, or when there are no paths qualified for an extension. If the objective function does not exceed the real cost of getting to the destination, A-star will always return the minimum cost path from start to destination.\n\nCONCLUSION\n\nA-Star has the shortest routing path with an average delay of 222.22ms and an average data transmission rate of 35.40Mbps, but ACO shows more reliability in terms of performance with a higher average data transmission rate of 48.42Mbps but an average delay of 799.07ms.\n\n\n\n\n\nReference list\nBrownlee, J., 2021. Local Optimization Versus Global Optimization [online]. MachineLearningMastery.com. Available from: https://machinelearningmastery.com/local-optimization-versus-global-optimization/#:~:text=Local%20optimization%20involves%20finding%20the.\nHocking, D., NouguÃ©s, J. M., RodrÃ­guez, J. C. and Sama, S., 2002. Chapter 3.3 - Simulation, Design & Analysis [online]. ScienceDirect. Available from: https://www.sciencedirect.com/science/article/abs/pii/S1570794602800104 [Accessed 12 Jan 2023].\nNocedal, J. and Wright, S. J., 2006. Introduction. Springer Series in Operations Research and Financial Engineering, 1â€“9.\nWikipedia Contributors, 2019. A* search algorithm [online]. Wikipedia. Available from: https://en.wikipedia.org/wiki/A.\nZhang, J., Xiang, L., Liu, D., Cui, J., Ng, S. X., Maunder, R. G., Graeupl, T., Carsten-Fiebig, U. and Hanzo, L., 2021. Semi-Stochastic Aircraft Mobility Modelling for Aeronautical Networks: An Australian Case-Study Based on Real Flight Data. IEEE Transactions on Vehicular Technology [online], 70 (10), 10763â€“10779. Available from: https://ieeexplore.ieee.org/document/9511787 [Accessed 12 Jan 2023].\n\n"
  },
  {
    "name": "Credit-Card-Classification-Project",
    "description": "Develop an accurate, fair, and transparent credit scoring system using the Random Forest Classifier algorithm.",
    "url": "https://github.com/adescofaj/Credit-Card-Classification-Project",
    "language": "Jupyter Notebook",
    "topics": [],
    "stars": 0,
    "forks": 0,
    "created_at": "2023-06-08T12:45:50Z",
    "updated_at": "2023-06-08T13:16:34Z",
    "homepage": null,
    "is_fork": false,
    "readme": null
  }
]